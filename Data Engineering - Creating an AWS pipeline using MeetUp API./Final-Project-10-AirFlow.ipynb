{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - MeetUp - Airflow Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Installing Airflow on EC2.\n",
    "\n",
    "Most of the steps have been taken from this tutorial:\n",
    "\n",
    "http://site.clairvoyantsoft.com/installing-and-configuring-apache-airflow/\n",
    "\n",
    "Since `python` and `pandas` are already installed, we install these:\n",
    "\n",
    "`#Build Essentials - GCC Compiler\n",
    "sudo apt-get install build-essential`\n",
    "\n",
    "`#SASL\n",
    "sudo apt-get install libsasl2-dev`\n",
    "\n",
    "`sudo pip install airflow==1.7.0\n",
    "sudo pip install airflow[hive]\n",
    "sudo pip install airflow[celery]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following directory will be created under `/home/ubuntu`:\n",
    "\n",
    "### `/home/ubuntu/airflow`\n",
    "\n",
    "<img src=\"images/air-1.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We go to the `airflow` directory and run:\n",
    "\n",
    "`airflow initdb`\n",
    "\n",
    "Also, we need to create two new directories under `/home/ubuntu/airflow`:\n",
    "\n",
    "`mkdir dags\n",
    "mkdir logs`\n",
    "\n",
    "<img src=\"images/air-2.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a `airflow.cfg` file is created.\n",
    "\n",
    "We edit that configuration file (`sudo nano airflow.cfg`) and make the following changes:\n",
    "\n",
    "`dags_are_paused_at_creation = True\n",
    "load_examples = False`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We start the web server:\n",
    "\n",
    "`nohup airflow webserver $* >> ~/airflow/logs/webserver.logs &`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We start the scheduler:\n",
    "\n",
    "`nohup airflow scheduler >> ~/airflow/logs/scheduler.logs &`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also start the web server as:\n",
    "\n",
    "`airflow webserver -p 8080`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On AWS, security groups, we need to enable inbound TCP traffic for port 8080 for any IP address.\n",
    "\n",
    "Then, we go to:\n",
    "\n",
    "http://ec2-54-91-189-236.compute-1.amazonaws.com:8080/admin/\n",
    "\n",
    "<img src=\"images/air-3.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### To test Airflow, we load a python script `tutorial.py` into the `/home/ubuntu/airflow/dags` folder.\n",
    "\n",
    "Then, on EC2, we run:\n",
    "\n",
    "`airflow initdb\n",
    "airflow webserver\n",
    "airflow scheduler`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We create a script `airflow_s3_report.py`.\n",
    "\n",
    "`scp -i ~/ssh/dsci6007_cpm.pem /Users/carles/airflow_s3_report.py ubuntu@ec2-54-91-189-236.compute-1.amazonaws.com:~/airflow/dags`\n",
    "\n",
    "##### Note that the script needs to live on `/home/ubuntu/airflow/dags` folder.\n",
    "\n",
    "Once we `scp` the script, we need to run again:\n",
    "\n",
    "`airflow initdb\n",
    "airflow webserver\n",
    "airflow scheduler`\n",
    "\n",
    "so the script is installed.\n",
    "\n",
    "<img src=\"images/air-4.jpg\" />\n",
    "<img src=\"images/air-5.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On EC2, we need to install:\n",
    "\n",
    "`sudo pip install iso8601`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE on the script `airflow_s3_report.py`.\n",
    "### IMPORTANT!! Even though the python executable script lives\n",
    "### in /home/ubuntu, we do not specify the bash_command as:\n",
    "### bash_command='/home/ubuntu email_s3_report.py '\n",
    "### BUT instead as:\n",
    "### bash_command='~/./email_s3_report.py '\n",
    "\n",
    "`python_s3_bucket_report = BashOperator(`\n",
    "\n",
    "  `task_id='python_s3_bucket_report',`\n",
    "  \n",
    "  `bash_command='~/./email_s3_report.py '`\n",
    "  \n",
    "  `+ bucket_input,`\n",
    "  \n",
    "  `dag=dag`\n",
    "  \n",
    "`)`\n",
    "\n",
    "reference: http://stackoverflow.com/questions/40146934/how-to-run-bash-script-file-in-airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note also that `email_s3_report.py` is `chmod a+x` to make it as python executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [dsci6007]",
   "language": "python",
   "name": "Python [dsci6007]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
