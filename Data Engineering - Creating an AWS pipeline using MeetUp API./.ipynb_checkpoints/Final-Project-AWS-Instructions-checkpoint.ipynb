{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - AWS Instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EC2 instance.\n",
    "\n",
    "<img src=\"images/ec2.jpg\" />\n",
    "\n",
    "#### Log into EC2:\n",
    "\n",
    "`ssh -i ~/ssh/dsci6007_cpm.pem ubuntu@ec2-54-91-189-236.compute-1.amazonaws.com`\n",
    "\n",
    "#### scp files to EC2:\n",
    "\n",
    "`scp -i ~/ssh/dsci6007_cpm.pem /Users/carles/api_meetup_cred.yml ubuntu@ec2-54-91-189-236.compute-1.amazonaws.com:~`\n",
    "\n",
    "`scp -i ~/ssh/dsci6007_cpm.pem /Users/carles/meet_up_kinesis.py ubuntu@ec2-54-91-189-236.compute-1.amazonaws.com:~`\n",
    "\n",
    "#### Install `requests` to run the script that loads data from MeetUp to S3 using kinesis firehose.\n",
    "\n",
    "`sudo pip install requests`\n",
    "\n",
    "#### Install `mailutils` to run the script sends an email with S3 bucket information.\n",
    "\n",
    "`sudo apt install mailutils`\n",
    "\n",
    "#### Other software installed:\n",
    "\n",
    "`sudo apt-get install python-dev python-pip`\n",
    "\n",
    "`sudo apt-get install python-pandas`\n",
    "\n",
    "`sudo apt-get install python-yaml`\n",
    "\n",
    "`sudo apt install awscli`\n",
    "\n",
    "`sudo pip install boto3`\n",
    "\n",
    "`sudo pip install pandas`\n",
    "\n",
    "`sudo pip install boto`\n",
    "\n",
    "`sudo apt install libpq-dev python-dev`\n",
    "\n",
    "`sudo pip install psycopg2`\n",
    "\n",
    "`sudo apt install mailutils`\n",
    "\n",
    "##### To view all software installed: `history | grep install`\n",
    "\n",
    "#### In case something needs to be deleted from S3 (CAUTION!!).\n",
    "\n",
    "`aws s3 rm --recursive s3://dsci6007-firehose-final-project/meet_up2017`\n",
    "\n",
    "#### To check the amount of data collected from MeetUp using kinesis firehose.\n",
    "\n",
    "`aws s3 ls --summarize --human-readable --recursive s3://dsci6007-firehose-final-project/meet_up2017`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Script to `GET` raw json from MeetUp into S3 (runs on above EC2 instance):\n",
    "\n",
    "`meet_up_kinesis.py`\n",
    "\n",
    "It is limited to 200 requests per hour to avoid been `throttle out` by MeetUp.\n",
    "\n",
    "#### CRON job.\n",
    "\n",
    "We will run the above script on EC2 every hour using CRON.\n",
    "\n",
    "We make the python script executable, we don't need to include python on the cron job:\n",
    "\n",
    "`chmod a+x meet_up_kinesis.py`\n",
    "\n",
    "scp script to EC2:\n",
    "\n",
    "`scp -i ~/ssh/dsci6007_cpm.pem /Users/carles/meet_up_kinesis.py ubuntu@ec2-54-91-189-236.compute-1.amazonaws.com:~`\n",
    "\n",
    "We create a CRON text file:\n",
    "\n",
    "`cron_kinesis_meetup.txt`\n",
    "\n",
    "which includes the following:\n",
    "\n",
    "`0 * * * * /home/ubuntu/meet_up_kinesis.py && curl -sm 30 k.wdt.io/carles.poles@gmail.com/meetup_firehose_s3?c=0_*_*_*_*`\n",
    "\n",
    "scp text file to EC2:\n",
    "\n",
    "`scp -i ~/ssh/dsci6007_cpm.pem /Users/carles/cron_kinesis_meetup.txt ubuntu@ec2-54-91-189-236.compute-1.amazonaws.com:~`\n",
    "\n",
    "Then, run this on EC2 AWS CLI:\n",
    "\n",
    "`crontab cron_kinesis_meetup.txt`\n",
    "\n",
    "### Note that we use the free utility  from `https://crontab.guru` to get email alerts from the above CRON.\n",
    "\n",
    "<img src=\"images/cron-guru.jpg\" />\n",
    "<img src=\"images/w-1.jpg\" />\n",
    "<img src=\"images/w-2.jpg\" />\n",
    "<img src=\"images/w-3.jpg\" />\n",
    "\n",
    "### We get an email every hour with information of the size of our firehose bucket.\n",
    "\n",
    "The script is `email_s3_report.py`.\n",
    "\n",
    "We make the python script executable, we don't need to include python on the cron job:\n",
    "\n",
    "`chmod a+x email_s3_report.py`\n",
    "\n",
    "scp script to EC2:\n",
    "\n",
    "`scp -i ~/ssh/dsci6007_cpm.pem /Users/carles/email_s3_report.py ubuntu@ec2-54-91-189-236.compute-1.amazonaws.com:~`\n",
    "\n",
    "and include this entry on `crontab`:\n",
    "\n",
    "`0 * * * * /home/ubuntu/email_s3_report.py dsci6007-firehose-final-project`\n",
    "\n",
    "<img src=\"images/s3-email.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Permissions for S3 bucket.\n",
    "`{\n",
    "  \"Id\": \"Policy1487106243408\",\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Sid\": \"Stmt1487106238363\",\n",
    "      \"Action\": [\n",
    "        \"s3:GetObject\"\n",
    "      ],\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Resource\": \"arn:aws:s3:::dsci6007-firehose-final-project/*\",\n",
    "      \"Principal\": {\n",
    "        \"AWS\": [\n",
    "          \"*\"\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Firehose Setup.\n",
    "\n",
    "<img src=\"images/firehose.jpg\" />\n",
    "\n",
    "Note the S3 buffer size is set to 100MB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## AWS Spark EMR  instance.\n",
    "\n",
    "<img src=\"images/emr-1.jpg\" />\n",
    "\n",
    "Log into instance.\n",
    "\n",
    "`ssh -i ~/ssh/dsci6007_cpm.pem hadoop@ec2-52-202-107-142.compute-1.amazonaws.com`\n",
    "\n",
    "#### Instance has installed:\n",
    "\n",
    "`history | grep install`\n",
    "\n",
    "showing:\n",
    "\n",
    "`sudo pip install jupyter`\n",
    "\n",
    "`sudo pip install pandas`\n",
    "\n",
    "`sudo pip install boto3`\n",
    "\n",
    "#### URL for Zeppelin:\n",
    "\n",
    "`http://ec2-52-202-107-142.compute-1.amazonaws.com:8890/`\n",
    "\n",
    "#### URL for Jupyter:\n",
    "\n",
    "`http://ec2-52-202-107-142.compute-1.amazonaws.com:8888/?token=TOKEN`\n",
    "\n",
    "where `TOKEN` will be provided when running `pyspark` on the EMR command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### In case we need to `scp` files to EMR:\n",
    "\n",
    "`scp -i ~/ssh/dsci6007_cpm.pem ~/Downloads/meetup_test hadoop@ec2-52-202-107-142.compute-1.amazonaws.com:~`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "https://developers.google.com/chart/interactive/docs/gallery/map#geocoded-locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### We need to copy html report files created by the scripts that are dumped to the EMR master node to a S3 bucket:\n",
    "\n",
    "`spark_meetup_time_rdd.py`\n",
    "\n",
    "`spark_meetup_time_df.py`\n",
    "\n",
    "#### We create a .sh script that will run on the master node:\n",
    "\n",
    "`emr_html_s3.sh`\n",
    "\n",
    "which contains:\n",
    "\n",
    "`aws s3 cp /home/hadoop/spark-top-meetup-categories-pie.html s3://dsci6007.com/`\n",
    "\n",
    "`aws s3 cp /home/hadoop/spark-top-meetup-categories.html s3://dsci6007.com/`\n",
    "\n",
    "`aws s3 cp /home/hadoop/spark-top-meetup-topics-pie.html s3://dsci6007.com/`\n",
    "\n",
    "`aws s3 cp /home/hadoop/spark-top-meetup-topics.html s3://dsci6007.com/`\n",
    "\n",
    "`aws s3 cp /home/hadoop/spark-top-meetup-organizers-pie.html s3://dsci6007.com/`\n",
    "\n",
    "`aws s3 cp /home/hadoop/spark-top-meetup-organizers.html s3://dsci6007.com/`\n",
    "\n",
    "`aws s3 cp /home/hadoop/spark-meetings-map.html s3://dsci6007.com/`\n",
    "\n",
    "`aws s3 cp /home/hadoop/meetup_report_files.html s3://dsci6007.com/`\n",
    "\n",
    "#### Then, we have a CRON job that run on the master node:\n",
    "\n",
    "`crontab cron_copy_html_s3.txt`\n",
    "\n",
    "which contains:\n",
    "\n",
    "`0 */1 * * * sh /home/hadoop/emr_html_s3.sh`\n",
    "\n",
    "we `scp` the 2 files to the master node:\n",
    "\n",
    "`scp -i ~/ssh/dsci6007_cpm.pem emr_html_s3.sh hadoop@ec2-52-202-107-142.compute-1.amazonaws.com:~`\n",
    "\n",
    "`scp -i ~/ssh/dsci6007_cpm.pem cron_copy_html_s3.txt hadoop@ec2-52-202-107-142.compute-1.amazonaws.com:~`\n",
    "\n",
    "Then enable the CRON job by running:\n",
    "\n",
    "`crontab cron_copy_html_s3.txt`\n",
    "\n",
    "#### Note that we copy the html files to a S3 bucket that has been enabled as static website.\n",
    "\n",
    "<img src=\"images/static.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have another CRON job to check the timestamp of the html file reports.\n",
    "\n",
    "We make the python script executable, we don't need to include python on the cron job:\n",
    "\n",
    "`chmod a+x email_timestamp_files.py`\n",
    "\n",
    "scp script to EMR:\n",
    "\n",
    "`scp -i ~/ssh/dsci6007_cpm.pem /Users/carles/email_timestamp_files.py hadoop@ec2-52-202-107-142.compute-1.amazonaws.com:~`\n",
    "\n",
    "and this is the crontab entry:\n",
    "\n",
    "`0 */1 * * * /home/hadoop/email_timestamp_files.py`\n",
    "\n",
    "The script generates an email and a html page:\n",
    "\n",
    "https://s3-us-west-1.amazonaws.com/dsci6007.com/meetup_report_files.html\n",
    "\n",
    "<img src=\"images/mrt-1.jpg\" />\n",
    "<img src=\"images/mrt-2.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that since we run a CRON job on the EMR master node, we will be receiving emails.\n",
    "\n",
    "<img src = \"images/mail-1.jpg\" />\n",
    "\n",
    "We go to:\n",
    "\n",
    "`cd /var/spool/mail`\n",
    "\n",
    "then:\n",
    "\n",
    "`cat hadoop`\n",
    "\n",
    "<img src = \"images/mail-2.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Submit.\n",
    "\n",
    "`scp -i ~/ssh/dsci6007_cpm.pem spark_meetup_time_df.py hadoop@ec2-52-202-107-142.compute-1.amazonaws.com:~`\n",
    "\n",
    "`scp -i ~/ssh/dsci6007_cpm.pem spark_meetup_time_rdd.py hadoop@ec2-52-202-107-142.compute-1.amazonaws.com:~`\n",
    "\n",
    "#### we will run `spark_meetup_time_rdd.py` and `spark_meetup_time_df.py` from EMR as:\n",
    "\n",
    "`spark-submit spark_meetup_time_df.py s3a://dsci6007-firehose-final-project/meet_up2017/*/*/*/*`\n",
    "\n",
    "`spark-submit spark_meetup_time_rdd.py s3a://dsci6007-firehose-final-project/meet_up2017/*/*/*/*`\n",
    "\n",
    ">NOTE: we may need to run on EMR command line `unset PYSPARK_DRIVER_PYTHON` to avoid launching jupyter notebook.\n",
    "If jupyter notebooks are required, after finishing with `spark-submit` tasks, run `source ~/.bashrc` (refer to https://github.com/carlespoles/DSCI6007-student/blob/master/5.4%20-%20Spark%20Submit/Lab_5_4-VijethLomada-CarlesPolesMielgo/Lab_5_4-VijethLomada-CarlesPolesMielgo.ipynb)\n",
    "\n",
    "#### we run job scripts in a CRON job every two hours:\n",
    "\n",
    "`scp -i ~/ssh/dsci6007_cpm.pem cron_jobs_emr.txt hadoop@ec2-52-202-107-142.compute-1.amazonaws.com:~`\n",
    "\n",
    "`crontab cron_jobs_emr.txt`\n",
    "\n",
    "`0 */2 * * * spark-submit /home/hadoop/spark_meetup_time_df.py s3a://dsci6007-firehose-final-project/meet_up2017/*/*/*/*`\n",
    "\n",
    "`0 */2 * * * spark-submit /home/hadoop/spark_meetup_time_rdd.py s3a://dsci6007-firehose-final-project/meet_up2017/*/*/*/*`\n",
    "\n",
    "##### we can \"concatenate\" both jobs (when one finishes, the other starts):\n",
    "\n",
    "`0 */2 * * * spark-submit /home/hadoop/spark_meetup_time_df.py s3a://dsci6007-firehose-final-project/meet_up2017/*/*/*/* && spark-submit /home/hadoop/spark_meetup_time_rdd.py s3a://dsci6007-firehose-final-project/meet_up2017/*/*/*/*`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "`7/03/01 18:32:56 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)\n",
    "('Error: ', 'An error occurred while calling o104.saveAsTextFile.\\n: com.amazonaws.services.s3.model.AmazonS3Exception: We encountered an internal error. Please try again. (Service: Amazon S3; Status Code: 200; Error Code: InternalError; Request ID: 50D9DA0B0C6FFC74), S3 Extended Request ID: yLDCHTtdetGgfoBSLoRQFlpmhsEXDSqVXKJ4QX9zzTfMq8R7T1xwDPFVh0sbAGM5iDdJSap1GPU=\\n\\tat com.amazonaws.services.s3.AmazonS3Client.copyObject(AmazonS3Client.java:1571)\\n\\tat com.amazonaws.services.s3.transfer.internal.CopyCallable.copyInOneChunk(CopyCallable.java:146)\\n\\tat com.amazonaws.services.s3.transfer.internal.CopyCallable.call(CopyCallable.java:134)\\n\\tat com.amazonaws.services.s3.transfer.internal.CopyMonitor.copy(CopyMonitor.java:193)\\n\\tat com.amazonaws.services.s3.transfer.internal.CopyMonitor.call(CopyMonitor.java:147)\\n\\tat com.amazonaws.services.s3.transfer.internal.CopyMonitor.call(CopyMonitor.java:47)\\n\\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\\n\\tat java.lang.Thread.run(Thread.java:745)\\n')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per above, I decided to re-size the cluster:\n",
    "\n",
    "<img src=\"images/resize.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Another report on S3 bucket we run on EC2:\n",
    "\n",
    "### script is `email_s3_insert_graph_report.py`\n",
    "\n",
    "### Example usage: `python email_s3_insert_graph_report.py dsci6007-firehose-final-project`\n",
    "\n",
    "### Before that, we need a new DB: `dsci6007_s3_db`\n",
    "\n",
    "<img src=\"images/s3-db.jpg\" />\n",
    "\n",
    "#### we create this table:\n",
    "\n",
    "`-- Table: public.s3_bucket`\n",
    "\n",
    "`-- DROP TABLE public.s3_bucket;`\n",
    "\n",
    "`CREATE TABLE public.s3_bucket`\n",
    "\n",
    "`(`\n",
    "    bucket_name character varying COLLATE pg_catalog.\"default\",`\n",
    "    \n",
    "    `size bigint,`\n",
    "    \n",
    "    `timestamp_firehose timestamp without time zone,`\n",
    "    \n",
    "    `total_objects bigint`\n",
    "    \n",
    "`)`\n",
    "\n",
    "`WITH (`\n",
    "\n",
    "    `OIDS = FALSE`\n",
    "    \n",
    "`)`\n",
    "\n",
    "`TABLESPACE pg_default;`\n",
    "\n",
    "\n",
    "`ALTER TABLE public.s3_bucket`\n",
    "\n",
    "    `OWNER to dsci6007;`\n",
    "    3\n",
    "The script will also create two images that will be saved to EC2, then moved to a S3 web bucket:\n",
    "\n",
    "<img src=\"images/s3_size_plot.png\" />\n",
    "\n",
    "<img src=\"images/s3_objects_plot.png\" />\n",
    "\n",
    "### IMPORTANT: TO BE ABLE TO SAVE GENERATED IMAGES ON EC2, we need to edit `matplotlibrc` which is found on `/etc/matplotlibrc`:\n",
    "\n",
    "`cd /etc/`\n",
    "\n",
    "`sudo nano matplotlibrc`\n",
    "\n",
    "and change this line from:\n",
    "\n",
    "`backend      : TkAgg`\n",
    "\n",
    "to:\n",
    "\n",
    "`backend       : Agg`\n",
    "\n",
    "Then, `scp` the script to EC2:\n",
    "\n",
    "`scp -i ~/ssh/dsci6007_cpm.pem /Users/carles/email_s3_insert_graph_report.py ubuntu@ec2-54-91-189-236.compute-1.amazonaws.com:~`\n",
    "\n",
    "If we want to bring the images from EC2 to local Mac:\n",
    "\n",
    "`scp -i ~/ssh/dsci6007_cpm.pem ubuntu@ec2-54-91-189-236.compute-1.amazonaws.com:~/s3_objects_plot.png /Users/carles/`\n",
    "\n",
    "`scp -i ~/ssh/dsci6007_cpm.pem ubuntu@ec2-54-91-189-236.compute-1.amazonaws.com:~/s3_size_plot.png /Users/carles/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We could run the above script in another CRON job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`chmod a+x email_s3_insert_graph_report.py`\n",
    "\n",
    "`0 * * * * /home/ubuntu/email_s3_insert_graph_report.py dsci6007-firehose-final-project`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated report can be found here:\n",
    "\n",
    "https://s3-us-west-1.amazonaws.com/dsci6007.com/s3_report.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [dsci6007]",
   "language": "python",
   "name": "Python [dsci6007]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
